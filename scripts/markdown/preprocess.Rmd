---
title: "preprocess"
author: "A Buchard"
date: "October 3, 2016"
output: html_document
---

```{r, warning=F, message=FALSE}
set.seed(1234)
## Please note this script will be using data.table extensively
library(CWAnalysis) 
```


__Script short description :__ Automatized and reproducible script importing raw data of the ATEC questionnaires 1, questionnaire 2, the teacher questionnaire as well as the D2, SART, Lachaux and CW tasks. The data is then processed and various composite features are calculated. The processed data is then merged into a single data.table ordered by subject through their unique id.

No group analysis is done on this preprocessing stage.

## A. Data Files  
ATEC raw Data was contained in various files separating each questionaires and each tasks. This data is associated in all the file with a unique child identifier comprised by the first two letters of their first and lastname as well as their day and month of birth.
THe first step was to select the relevant raw data files, copy and rename them appropriatly. All the files were stored in a single folder called data.

```{r, warning=F}
## Set the correct path depending on where you copied to repo
setwd("~/Google Drive/dev/R/CWAnalysis/analysis/data")
pathToScriptFolder = "~/Google Drive/dev/R/CWAnalysis/analysis/scripts"
pathToDataFolder = "~/Google Drive/dev/R/CWAnalysis/analysis/data"
```

Here is a one to one correspondance list of the new and old raw data files :

* “rawDataSubjectsGeneralInformations.csv” = data_subjects.txt
* “rawDataCodedTasksAndQuestionnaires.csv” = tasks_subjects.xlsx
* "rawDataQuestionnaire1.csv" = Data_questionnaires_partie1.xlsx    
* "rawDataQuestionnaire2.csv" = Data_questionnaires_partie2 (flavia).xlsx
* "rawDataTaskD2.csv" = DATA_D2.xlsx
* "rawDataTaskLachaux.csv"  = NEW_Lachaux.txt            
* "rawDataTaskSart.csv" = NEW_SART.txt
* "rawDataQuestionnaireConnersTeacher.csv" = Data_questionnaires_teachers.xlsx

To complete those experimental data files, utility files were created in order to automatized data analysis :

* "activityCorrespondanceTable.csv" is a correspondance table permitting to classify various extracurriculum activity depending on their type and caracteristics
+ a normative string to correct typo and group activities
+ a type string
+ Booleans 0/1 for is it a sport, performing art, violent, or competitive

A description of each data file will be given as they are imported.

## 1. rawDataSubjectsGeneralInformations.csv
```{r, warning=F}
rawSubjectsGeneralInformations = read.csv(file.path(pathToDataFolder, "raw/general_informations/rawDataSubjectsGeneralInformations.csv"))
```

This files contain various general information for each subject, as follows :

```{r, echo=F, warning=F}
colnames(rawSubjectsGeneralInformations)
```

Please note that there are 4 levels in "category", as follows :

```{r, echo=F, warning=F}
unique(rawSubjectsGeneralInformations$category)
```

sp represents children in a _special_ class, with additional educational support. Those children will be analysed separatly.

#### Processing data

```{r, echo=F, warning=F}
processedSubjectsGeneralInformations = as.data.table(rawSubjectsGeneralInformations)
```

Variables will be renamed in the final dataframe format as follows :

```{r, echo=F, warning=F}
setnames(processedSubjectsGeneralInformations, c("id", "dateOfBirth", "age", "gender", "schoolYear", "teacherId", "group"))
colnames(processedSubjectsGeneralInformations)
processedSubjectsGeneralInformations[age>11,]
```

* "kid.id" to id     
* "date.naiss." to "dateOfBirth"
* "age" to "age"
* "sex" to "gender"
* "level" to "schoolYear"
* "teacher" to "teacherId"
* "category" to "group"


Gender will be converted into : male female.

```{r, echo=F, warning=F}
levels(processedSubjectsGeneralInformations$gender) = list(male = "boy", female = "girl") 

```


Group will be : "8to9yo", "10yo", "11to12yo", "specialClass"

```{r, echo=F, warning=F}
processedSubjectsGeneralInformations$group = factor(processedSubjectsGeneralInformations$group, levels = c("8n9", "10","11n12","sp"), labels = c("8to9yo", "10yo", "11to12yo", "specialClass"))
```

Final processed information is structured as so :

```{r, warning=F}
colnames(processedSubjectsGeneralInformations)
```


## 2. rawDataCodedTasksAndQuestionnaires.csv
```{r, warning=F}
rawCodedTasksAndQuestionnaires = read.csv(file.path(pathToDataFolder, "raw/questionnaires/rawDataCodedTasksAndQuestionnaires.csv"))
```

This files contain information about which tasks or questionnaire were performed/taken for each subject, as follows :

```{r, echo=F, warning=F}
str(rawCodedTasksAndQuestionnaires)
```

Note that the accomplished tasks or questionnaires doone bz each subject is marked in the appropriate cell by an "x" character.

Questionnaire 1 is the questionnaire comprising General Questions, Theorz of intelligence, Sleep, Media Use, Multimedia Use, Multitasking and Video Games.
Questionnaire 2 is the questionnaire comprising Grit, Well-Being, Extra-curriculum activities, Conners Kids and Conners parents.
Questionnaire Teacher is the Conners for teachers.

### Processing data

```{r, echo=F, warning=F}
processedCodedTasksAndQuestionnaires = as.data.table(rawCodedTasksAndQuestionnaires)
```

Two features will be dropped from the data as they are already coded in rawSubjectsGeneralInformations : category, and Sex. Leaving 9 features and an id column.

```{r, warning=F}
processedCodedTasksAndQuestionnaires[, c("category", "Sex"):=NULL]
```

Variables will be renamed in the final dataframe format as follows :

```{r, echo=F, warning=F}
setnames(processedCodedTasksAndQuestionnaires, 
         c("id", "doneMot" ,"doneLachaux1", "doneLachaux2", "doneSart1", "doneSart2", "doneD2", "doneQuestionnaireTeacher", "doneQuestionnaire2", "doneQuestionnaire1"))
```

* "kid.id" to id     
* MOT to "doneMot"
* Lacho1 to "doneLachaux1"
* Lacho2 to "doneLachaux2"
* Sart1 to "doneSart1"
* Sart2 to "doneSart2"
* D2 to "doneD2"
* q_teach to "doneQuestionnaireTeacher"
* q2 to "doneQuestionnaire2"
* q1 to "doneQuestionnaire1"



Task features will be transformed into binary factor containing either 0 for not done '', and 1 for done 'x'.
```{r, echo=F, warning=F}
cnames <- colnames(processedCodedTasksAndQuestionnaires[, -"id", with=FALSE]) ## or [-1]
processedCodedTasksAndQuestionnaires[, (cnames) := lapply(.SD, function(feature) { factor(feature, labels = c(0,1)) }), .SDcols = (cnames)]
```


Final processed information is structured as so :

```{r, warning=F}
colnames(processedCodedTasksAndQuestionnaires)
```


## 3. rawDataQuestionnaire1.csv
```{r, warning=F}
rawQuestionnaire1 = read.csv(file.path(pathToDataFolder, "raw/questionnaires/rawDataQuestionnaire1.csv"))
```

This files contains all answered data from questionnaire 1 for each subject, as follows :

```{r, echo=F, warning=F}
#str(rawQuestionnaire1, list.len = 500)
```

A short description is given here - please refer to attached questionnaire pdf for more detailed info :

* sid : unique child id
* weight, height, handedness, sex, brotherhood_size, brotherhood_rank : self explaining
* health self rating about own health
* good_results self rating about own results in school
* confident_find_job  self rating about confidience to find a job in the future
* grades self rating about mean score in school [1 - 6]
* languages_home list of language spoken at home, string format separated with commas
* nb_languages_home self exp
* pb_see yes/no string coding subjects problem seeing
* pb_hear yes/no string coding subjects problem hearing
* pb_expression yes/no string coding subjects problem expressing himself
* pb_learn yes/no string coding subjects problem learning
* potentials_pb string coding for other possible problems
* TOI
+ Theory of Intelligence scale as adapted from Dweck, C. S. (2006). Mindset: The new psychology of success.
+ ref in media/documentation folder
* sleep
+ dreams, nightmare 1-4
+ 7 sleep related questions
* media
+ A series of question characterizing media consumption 
* multitask
+ A series of questions characterizing multiple media consumption at once
+ Group by one fixed media type with varying concurrent medias
+ Series 1 while reading
+ Series 2 while looking at videos
+ Series 3 while listening to music
+ Series 4 while writing
+ Series 5 while creating media - music, drawing, films..
+ Series 6 while playing video games
+ Series 7 while looking at information on the web
+ Series 8 while speaking over the phone or the internet
* Devices
+ nb_computers, nb_tv, nb_tablets, nb_smartphone, nb_consoles, nb_cons_portables : number of each devices in the child's room
+ appareils total number of devices
* video_game
+ 5 video games were possible to code for each three variable N is [1-5]
+ video_gameN name of video game
+ video_gameN_cons which console
+ video_gameN_duration how much time played three groups rarely 1 , a lot 3

Please note that one variable is not named and this variable is named X automatically by the str function displaying the preceding data. This variable and the following one, named new_problem does not correspond to any apparent question in the questionnaire, 

### Processing data

```{r, echo=F, warning=F}
processedQuestionnaire1 = as.data.table(rawQuestionnaire1)
## delete the one empty row
processedQuestionnaire1 = processedQuestionnaire1[sid!="",] 
```

Some featured will be dropped as they've been coded in previous imports, or have no use: sex, potentials_pb, X, new_problem.

```{r, warning=F}
processedQuestionnaire1[, c("sex", "potentials_pb", "X", "new_problem") := NULL]
```

Variables will be renamed in the final dataframe. Format as follows :

```{r, warning=F}
setnames(processedQuestionnaire1, c("sid", "appareils","jeuvideo2_temps"), c("id", "nbTotalDevices", "video_game2_duration"))
```

Problems will be simplified with a composite variable called NumberOfProblems

```{r, warning=F}
cnames = c("pb_hear", "pb_see", "pb_expression", "pb_learn"); 

processedQuestionnaire1[, (cnames) := lapply(.SD, function(feature) { 
    as.numeric(as.character(factor(feature, levels = c("yes","no"), labels = c(1,0) ))) }), .SDcols = (cnames)]

processedQuestionnaire1[, numberOfProblems := sum(pb_hear, pb_see, pb_learn, pb_expression, na.rm = T), by=id]
```

A grouping variable differencing kids with more than one language at home

```{r, warning=F}
processedQuestionnaire1[nb_languages_home==1, moreThanOneLanguageAtHome := FALSE]
processedQuestionnaire1[nb_languages_home>1, moreThanOneLanguageAtHome := TRUE]
```

Composite variable for grade satisfaction : gradeSatisfaction. Computed as (good_results * maxGrade / grades * maxResults)

```{r,warning=F,message=F}
processedQuestionnaire1[, gradeSatisfaction := ((good_results*6)/(grades*4)), by="id"]
```

In the raw data :

* TOI
    + TOI_fixed = score à l’axe fixed 
    + TOI_growth = score à l’axe growth 
    + TOI_score = score total au test 
* sleep_score : score total au test 
    + q3 est q6 are positive, orthers negatives
    + score is calculated in order to obtain a positively correlated score to sleep quality
* media
    + all ints 1-4 are converted into time in the newData 0-4.5
    + 0 - 0.5 - 1.5 - 3 - 4.5
    + Week_time = temps total pour la semaine 
    + Weekend _time = temps total pour le week-end
* Multitask_score = score total de multitasking 
* Total_devices= renamed from appareils
* Games
    + Game_code 
    + 0 : no game, 1 : game, 2 : shooter/RTS/sports 
+ Duration
    + 1 = Rarement 2 = Parfois  3 = Beaucoup

#### TOI Score

    Questions
        1. ability mindset – fixed
        2. ability mindset –growth
        3. ability mindset – growth
        4. personality/character mindset - fixed
        5. personality/character mindset – growth
        6. ability mindset – growth
        7. ability mindset – fixed
        8. ability mindset – fixed
        9. ability mindset – growth
        10. personality/character mindset - growth 
        11. ability mindset – fixed
        12. personality/character mindset – fixed 
        13. ability mindset –growth
        14. ability mindset – fixed
        15. ability mindset – growth
        16. ability mindset – fixed
        17. personality/character mindset – fixed
        18. personality/character mindset –growth 
        19. ability mindset – growth
        20. ability mindset - fixed

Scoring
    Growth Questions
        1. Strongly agree – 3 points
        2. Agree – 2 points
        3. Disagree – 1 points
        4. Strongly disagree – 0 point
    Fixed Questions
        1. Strongly agree – 0 point
        2. Agree – 1 points
        3. Disagree – 2 points
        4. Strongly disagree – 3 points
        
Strong Growth Mindset = 60-45 points
Growth Mindset with some Fixed ideas = 44-34 points
Fixed Mindset with some Growth ideas= 33-21 points
Strong Fixed Mindset = 20-0 points
               

First we recalculate the TOI scores through a custom function called atecFuncAddTOIComposites, documented in the atecUtil.R utility file:

```{r processedQuestionnaire1::atecFuncAddTOIComposites, warning=F}
processedQuestionnaire1 = atecFuncAddTOIComposites(processedQuestionnaire1)


```

#### Sleep Score

We then caculate the sleep score, only 4 children did not fill correctly this part of the questionnaire (n=131 - 4) :
```{r processedQuestionnaire1::sleepScore, warning=F}
processedQuestionnaire1[, sleepScore := (13+(sleep3+sleep6-sleep1-sleep2-sleep4-sleep5-sleep7))/14]

```


#### MMI : Multitask media index 

Caulculated from "Cognitive control in media multitasker" -  Ophir, Wagner


```{r processedQuestionnaire1::mediaTimeSpent, warning=F}

# Chat without technology is not used in the computation of the Media use during week and weekends as well as the MMI

multitaskmediaTypesColnames = c("Read",
                                "Video", 
                                "Music", 
                                "Write", 
                                "Create", 
                                "VG", 
                                "InfoInternet", 
                                "ChatWithTech",
                                "ChatNoTech")

newNameTimePerMediaWeek = paste0("media", multitaskmediaTypesColnames, c("TimeWeek"))
newNameTimePerMediaWeekend = paste0("media", multitaskmediaTypesColnames, c("TimeWeekend"))
oldNameTimePerMediaWeek = paste0("media1.", 1:9)
oldNameTimePerMediaWeekend = paste0("media2.", 1:9)

setnames(processedQuestionnaire1, 
         c(oldNameTimePerMediaWeek,oldNameTimePerMediaWeekend), 
         c(newNameTimePerMediaWeek, newNameTimePerMediaWeekend))

# cnames = paste0("media", 1:2, ".", c(1:9, 1:9))
cnames = c(newNameTimePerMediaWeek[1:8], newNameTimePerMediaWeekend[1:8])

processedQuestionnaire1[, (cnames) := lapply(.SD, function(feature) { 
    as.numeric(as.character(factor(feature, levels = c(1,2,3,4,5), labels = c(0,0.5,1.5,3,4.5) ))) }), .SDcols = (cnames)]


cnames = newNameTimePerMediaWeek[1:8]
processedQuestionnaire1[, mediaSpentTimeDuringWeek := rowSums(.SD, na.rm = TRUE), .SDcols = cnames]
cnames = newNameTimePerMediaWeekend[1:8]
processedQuestionnaire1[, mediaSpentTimeDuringWeekend := rowSums(.SD, na.rm = TRUE), .SDcols = cnames]

processedQuestionnaire1[, mediaSpentTimeTotal := (5*mediaSpentTimeDuringWeek+2*mediaSpentTimeDuringWeekend)/7]

## VideoGame Objective Time
vgWeek = newNameTimePerMediaWeek[6]
vgWeekend = newNameTimePerMediaWeekend[6]

processedQuestionnaire1[, videoGameTimeSpentTotalPerWeek:= (5*get(vgWeek)+2*get(vgWeekend))]
processedQuestionnaire1[, videoGameTimeSpentTotalPerDay:= (5*get(vgWeek)+2*get(vgWeekend))/7]
processedQuestionnaire1[, videoGameHighUser:= videoGameTimeSpentTotalPerWeek>5]

processedQuestionnaire1[, videoGameTimeSpentTotalPerWeekWinsorized := Winsorize(videoGameTimeSpentTotalPerWeek, na.rm = T)]

#' Get column names for each media in the multitask media use questionnaire 

load(file.path(pathToDataFolder, "utility/multitasking/multitaskMediaColsToAddMatrix.Rdata"))

## Delete Chat without technology
multitaskMediaColsToAddMatrix = multitaskMediaColsToAddMatrix[-9,-8]
## Refactor each variable to obtain an index from 0 to 1    
cnames = unique(paste0(multitaskMediaColsToAddMatrix))
processedQuestionnaire1[, (cnames) := lapply(.SD, function(feature) 
{ 
    as.numeric(as.character(factor(feature, levels = c(1,2,3), labels = c(0,0.5,1) )))
}), 
.SDcols = cnames]
```

### MMI Score

Calculated by the function atecFuncGetMMIscore in atecUtils.R
Formula from "Cognitive control in media multitasker" -  Ophir, Wagner

* Description
    * The article tries to determine if light and heavy multitasks process information differently and characterize those systematic differences. 
* Media use questionnaire and MMI 
    * administered online
    * Dress 12 media forms
    * Print media, television, computer based video (youtube, tv series), music, non music audio, video, computer games telephone, mobile phone voice calls, instant messaging, sms, email, web surfing, other computer based application.
        * they filled the media multitasking matrix, indicating weather while using the primary media they concurrently use the secondary form of media. 
        
* How to score it
    * Assign numeric value to each of the matrix response a follow
        * most of the time = 1
        * some of the time - 0.67
        * a little of the time = 0.33
        * never = 0
    * Sum the response for each primary media. 
    *  this gives a mean measure of the mean number of media used while using each primary medium. 
    * Weighted for all primary media with the time spent with each media to account for the different amount of time spent on each medium. 
    * divided by the total amount of time spent with ALL primary medium
    * MMI = sum over all medium I(∑Mij * (5*WeekTimeMediaI + 2* WeekendTimeMediaI / 7)) / mediaSpentTimeTotal
        * Mij = score usage of media j while using medium i = The sum represent the average number of media used while using media i
        * WeekTimeMediaI, WeekendTimeMediaI, and mediaSpentTimeTotal are calculated from the media questionnaire above (newNameTimePerMediaWeek)
    
Questions of questionnaire 1 permits to produce an interaction matrix (adjacency matrix) between all media types for multitasking. Each question list reffering to a particular media have been isolated in a 8 by 8 matrice (diagonal has been suppressed) containing the names of the question variable.  

```{r processedQuestionnaire1::multitask, warning=FALSE}
# Chat without technology was not used
multitaskmediaTypesColnames = c("Read",
                                "Video", 
                                "Music", 
                                "Write", 
                                "Create", 
                                "VG", 
                                "InfoInternet", 
                                "ChatWithTech")


# atecFuncGetMMIScore is in atecUtils.R
processedQuestionnaire1 = atecFuncGetMMIscore(processedQuestionnaire1, 
                                              multitaskMediaColumnsMatrix = multitaskMediaColsToAddMatrix,
                                              mediaTypes = multitaskmediaTypesColnames,
                                              timeColnamesWeek = newNameTimePerMediaWeek,
                                              timeColnamesWeekend = newNameTimePerMediaWeekend)


```


#### Game Features 
Each game was characterized in the utility/video_games/gameCharacteristicsActionCoded.csv file. See documentation of the atecFuncCreateVideoGamesFeatures() in atecUtils.R for details on how game features are computed.


```{r processedQuestionnaire1::videoGamePerceivedAmountPlayed, warning=F}
processedQuestionnaire1[, videoGameNames := paste0(video_game1, ",", video_game2, ",", video_game3, ",",video_game4), by=id]
processedQuestionnaire1[, videoGameConsoles := paste0(video_game1_cons, ",", video_game2_cons, ",", video_game3_cons, ",",video_game4_cons), by=id]
processedQuestionnaire1[, 
                        videoGamePerceivedAmountPlayedConcatenated := 
                            paste0(video_game1_duration, ",",video_game2_duration, ",",video_game3_duration, ",",video_game4_duration), by=id]

#TODO check gamenames
## We exports the unique names of videogame in a csv file. 
## It was done once, previously - the code is kept here for archive

### processedQuestionnaire1[, videoGameNames := paste0(video_game1, ",", video_game2, ",", video_game3, ",",video_game4), by=id]
### gameNames = paste0(processedQuestionnaire1[,videoGameNames], collapse = ",")

### Always weird bugs, here stating the encoding of the character vector in necessary for spliting

### Encoding (gameNames) <- "latin1"

### gameNamesUniques = unique(unlist(strsplit(gameNames, ",")))
### write.csv(gameNamesUniques, file = "gameNamesUniques.csv")

### We then need to extract information for each game and place it is new composite features

```

Only two outliers have videogameNames without durations encoded.
There is a subject varo2604 that partially filled the questionnaire / or data has been badly encoded. 
Prevent from corruption we set videogames to NA



```{r processedQuestionnaire1::atecFuncCreateVideoGamesFeatures, warning=F}
processedQuestionnaire1[is.na(videoGamePerceivedAmountPlayedConcatenated)&!is.na(videoGameNames), c("videoGamePerceivedAmountPlayedConcatenated","videoGameNames"), with=F]

processedQuestionnaire1[id=="varo2604", videoGameNames:=NA] 

game_data = read.csv(file.path(pathToDataFolder, "utility/video_games/gameCharacteristicsActionCoded.csv"))
gameCompositesColums = atecFuncCreateVideoGamesFeatures(processedQuestionnaire1, 
                                                        nameColumn = "videoGameNames",
                                                        gameData = game_data)

processedQuestionnaire1 = cbind(processedQuestionnaire1, gameCompositesColums)
```

Final processed information is structured as so :

```{r, warning=F}
str(processedQuestionnaire1)
```


### 4. rawDataQuestionnaire2.csv
```{r, warning=F}
rawQuestionnaire2 = read.csv(file.path(pathToDataFolder, "raw/questionnaires/rawDataQuestionnaire2.csv"))
```

Only 98 subjects took this questionnaire. A lot is not clearly known about the processing of the file, and reuse of the previously processed data held in "NEW_Data_q2.txt" should be discussed if proper documentation is not at disposal. 

This file contains all answered data from questionnaire 2 for each subject, as follows :

```{r, echo=F, warning=F}
colnames(rawQuestionnaire2)
```

Second questionnaire has to with :

* Personnality/Grit
    + 24 question on different personnality traits coded as "grit" 
    + Three questionnaires a present in this first questionnaire :
    + The first twelve questions are measuring grit
    + Questions 1 4 6 9 10 12 are coded positively
    + Questions 2 3 5 7 8 11 are coded negatively
    + Ref original version:
        + Duckworth, A.L., Peterson, C., Matthews, M.D., & Kelly, D.R. (2007). Grit: Perseverance and passion for long-term goals. Journal of Personality and Social Psychology, 9, 1087-1101
+ Question 13 through 19 are not part of a validated scale : VariousPersonalityQuestion
    + 19 I am easilly distracted
    + 18 I manage to do several things at once
    + 17 I like to use severall technologies at the same time
    + 16 I pass too much time using technologies
    + 15 I get bored fast
    + 14 I learn fast
    + 13 I am good at using new technologies
+ Question from 20 to 24 are a translation of the MWQ scale measuring mind wondering 
    + The response interval has been shrunk from [1-6] to [1-5] ans will need to be resized to be shared adequatly with others.
    + http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00560/abstract
 
* Well Being / Depression 
    + 6 question coded "welfare"
    + Score is the sum of the result on all question
    + Inversely correlated to well being : The greater the score is the less happy the subject is 
    + Not calculated in the raw data
    + Short Question Description
        + welfare1   nervousess
        + welfare2   hopelessness
        + welfare3   agitated
        + welfare4   depressed
        + welfare5   aboulia
        + welfare6   low self esteem
* Extra curriculum activities
    + 5 Extracurricular activity were coded
    + by a namestring in extra_activityN feature
    + by weekly frequency in extra_activityN_frequency 
    + and duration of each session in hours in extra_activityN_duration 

#### Processing data

```{r, echo=F, warning=F}
processedQuestionnaire2 = as.data.table(rawQuestionnaire2)
```

The code column needs to be renamed to id in order to merge it properly, and other columns will be renamed as follows :

```{r, warning=F}
gritQuestionsNames = paste0("gritQ",1:12)
variousPersonnalityQuestionNames = paste0("variousPersonnalityQ",1:7)
mwqNames = paste0("mindWanderingMWQScaleQ",1:5)
personnalityTraitAllquestions = c(gritQuestionsNames, variousPersonnalityQuestionNames, mwqNames)

processedQuestionnaire2[, welfare_score := NULL]

setnames(processedQuestionnaire2, c("code", "grit_score"), c("id", "gritScore"))
setnames(processedQuestionnaire2, paste0("grit",1:12), gritQuestionsNames)
setnames(processedQuestionnaire2, paste0("grit",13:19), variousPersonnalityQuestionNames)
setnames(processedQuestionnaire2, paste0("grit",20:24), mwqNames)
setnames(processedQuestionnaire2, paste0("welfare",1:6), paste0("welfareQ",1:6))

sdq_questions_new_names = paste0("sdq_kids_q", 1:25)
setnames(processedQuestionnaire2, paste0("conners_kids",1:25), sdq_questions_new_names)
setnames(processedQuestionnaire2, "conners_kids_sore", "connersKidScore")
setnames(processedQuestionnaire2, paste0("conners_parents",1:48), paste0("connersParentQ",1:48))
setnames(processedQuestionnaire2, "conners_parents_score", "connersParentScore")

```


* Extracurriculum activities
    + extracurriculumActivitiesNames
    + extracurriculumWeeklyHours
    + extracurriculumDoingSport
    + extracurriculumdoingMusic
    + extracurriculumdoingCompetitiveActivity
    + extracurriculumdoingPerformingArts
* gritScore := positive - negative + 36 = 36 + (gritQ1+gritQ4+gritQ6+gritQ9+gritQ10+gritQ12-gritQ2-gritQ3-gritQ5-gritQ7-gritQ8-gritQ11)
* MWQ = mindWanderingMWQScaleScore = 1.26 * somme(questions)
* Welbeing = 30 - sum(questions)
* Conners: see file in documentation CWAnalysis/analysis/media/documentation/Conners_surveys 
* Questionnaire SDQ Enfant: see files in documentation CWAnalysis/analysis/media/documentation/SDQ_kids

```{r SDQ Kids recoding, message=F, warning=F, echo=F}

sdq_emotional_questions = c(3,8,13,16,24)
sdq_behavior_questions = c(5,7,12,18,22)
sdq_hyperactivity_questions = c(2,10,15,21,25)
sdq_relational_questions = c(6,11,14,19,23)
sdq_prosocial_questions = c(1,4,9,17,20)
sdq_negatively_score_questions = c(7,11,14,21,25)

# Questions seems to be coded exactly as entered on the questionnaire (no processing of the negative questions)
ggplot(processedQuestionnaire2, aes_string(x=sdq_questions_new_names[11], y= sdq_questions_new_names[14])) + geom_jitter()
ggplot(processedQuestionnaire2, aes_string(x=sdq_questions_new_names[14], y= sdq_questions_new_names[6])) + geom_jitter() 

# Recode the questions between 0 and 2 
cnames = sdq_questions_new_names[c(1:25)%nin%sdq_negatively_score_questions]
processedQuestionnaire2[, (cnames) := lapply(.SD, function(feature) 
{ 
    as.numeric(as.character(factor(feature, levels = c(1,2,3), labels = c(0,1,2) )))
}), 
.SDcols = cnames]

# Code negative questions between 2 and 0 
cnames = sdq_questions_new_names[sdq_negatively_score_questions]
processedQuestionnaire2[, (cnames) := lapply(.SD, function(feature) 
{ 
    as.numeric(as.character(factor(feature, levels = c(1,2,3), labels = c(2,1,0) )))
}), 
.SDcols = cnames]

# Compute scores
library(ppcor)
processedQuestionnaire2[, sdq_relational_score := sum(as.numeric(.SD)), .SDcols = sdq_questions_new_names[c(sdq_relational_questions)], by="id"]
processedQuestionnaire2[, sdq_hyperactivity_score := sum(as.numeric(.SD)), .SDcols = sdq_questions_new_names[c(sdq_hyperactivity_questions)], by="id"]
processedQuestionnaire2[, sdq_behavior_score := sum(as.numeric(.SD)), .SDcols = sdq_questions_new_names[c(sdq_behavior_questions)], by="id"]
processedQuestionnaire2[, sdq_emotional_score := sum(as.numeric(.SD)), .SDcols = sdq_questions_new_names[c(sdq_emotional_questions)], by="id"]
processedQuestionnaire2[, sdq_difficulty_score := sum(as.numeric(.SD)), .SDcols = sdq_questions_new_names[c(sdq_emotional_questions, sdq_behavior_questions, sdq_hyperactivity_questions, sdq_relational_questions)],by="id"]

processedQuestionnaire2[, sdq_prosocial_score := sum(as.numeric(.SD)), .SDcols = sdq_questions_new_names[c(sdq_prosocial_questions)], by="id"]

# pcor(processedQuestionnaire2[, c("sdq_relational_score", "sdq_hyperactivity_score", "sdq_behavior_score", "sdq_emotional_score","sdq_prosocial_score"), with=F])

```

```{r conners recoding, message=F, warning=F, echo=F}

conners_parent_questions = paste0("connersParentQ", 1:48)
conners_teacher_questions = paste0("connersTeacherQ", 1:28)

parent_questions_for_total_score = c(4,10,11,13,19,25,30,31,33,37)
teacher_questions_for_total_score = c(1,3,7,8,14,15,21,22,26,28)

conners_parent_questions_for_conduct = c(2,8,14,19,20,27,35,39)
conners_parent_questions_for_learning = c(10,25,31,37)
conners_parent_questions_for_psychosomatic = c(32,41,43,44)
conners_parent_questions_for_impulsivity = c(4,5,11,13)
conners_parent_questions_for_anxiety = c(12,16)


cnames = c(conners_parent_questions)
# All conner questions need to be recoded to scale from 0 to 3
processedQuestionnaire2[, (cnames) := lapply(.SD, function(feature) { 
    as.numeric(as.character(factor(feature, levels = c(1,2,3,4), labels = c(0,1,2,3) ))) }), .SDcols = (cnames)]



processedQuestionnaire2[, conners_parent_conduct_score := sum(as.numeric(.SD)), .SDcols= conners_parent_questions[conners_parent_questions_for_conduct], by="id"]

processedQuestionnaire2[, conners_parent_learning_score := sum(as.numeric(.SD)), .SDcols= conners_parent_questions[conners_parent_questions_for_learning], by="id"]

processedQuestionnaire2[, conners_parent_psychosomatic_score := sum(as.numeric(.SD)), .SDcols= conners_parent_questions[conners_parent_questions_for_psychosomatic], by="id"]

processedQuestionnaire2[, conners_parent_impulsivity_hyper_score := sum(as.numeric(.SD)), .SDcols= conners_parent_questions[conners_parent_questions_for_impulsivity], by="id"]

processedQuestionnaire2[, conners_parent_anxiety_score := sum(as.numeric(.SD)), .SDcols= conners_parent_questions[conners_parent_questions_for_anxiety], by="id"]

# get rid of the old column 
processedQuestionnaire2[, connersParentScore := NULL]

# recode 
processedQuestionnaire2[, conners_parent_score := sum(as.numeric(.SD)), .SDcols= conners_parent_questions, by="id"]

```

```{r, warning=F}

processedQuestionnaire2[, (personnalityTraitAllquestions) := lapply(.SD, function(feature) {
    as.numeric(as.character(feature)) }), .SDcols = (personnalityTraitAllquestions)]

NROW(na.omit(processedQuestionnaire2[,c("id",personnalityTraitAllquestions), with=F ]))
```

Only 74 subjects have filled the personality trait questions completely (a lot of "?").

```{r, warning=F}
processedQuestionnaire2[, gritScore := 36+(gritQ1+gritQ4+gritQ6+gritQ9+gritQ10+gritQ12-gritQ2-gritQ3-gritQ5-gritQ7-gritQ8-gritQ11)]

processedQuestionnaire2[, mindWanderingMWQScaleScore := 1.26*(mindWanderingMWQScaleQ1+mindWanderingMWQScaleQ2+mindWanderingMWQScaleQ3+mindWanderingMWQScaleQ4+mindWanderingMWQScaleQ5)]


```

Extracurriculum activites need to be processed in a representable form. A correspondance table was created using unique values from the questionnaire and coding various features as follows :

```{r, warning=F}
activityCorrespondanceTable = as.data.table(read.csv(file.path(pathToDataFolder, "utility/activities/activityCorrespondanceTable.csv")))
str(activityCorrespondanceTable)

processedQuestionnaire2[,extra_activity1_duration := as.numeric(levels(extra_activity1_duration))[extra_activity1_duration]]
processedQuestionnaire2[,extra_activity2_duration := as.numeric(levels(extra_activity2_duration))[extra_activity2_duration]]

processedQuestionnaire2[,extra_activity1_frequency := as.numeric(levels(extra_activity1_frequency))[extra_activity1_frequency]]
processedQuestionnaire2[,extra_activity2_frequency := as.numeric(levels(extra_activity2_frequency))[extra_activity2_frequency]]
processedQuestionnaire2[,extra_activity3_frequency := as.numeric(levels(extra_activity3_frequency))[extra_activity3_frequency]]
processedQuestionnaire2[,extra_activity4_frequency := as.numeric(levels(extra_activity4_frequency))[extra_activity4_frequency]]


processedQuestionnaire2[, extracurriculumActivitiesNames := paste0(extra_activity1,",",
                                                                   extra_activity2,",",
                                                                   extra_activity3,",",
                                                                   extra_activity4,",",
                                                                   extra_activity5,","), by = id]

processedQuestionnaire2[, extracurriculumActivitiesDurations := 
                            paste0(extra_activity1_duration*extra_activity1_frequency,",",
                                   extra_activity2_duration*extra_activity2_frequency,",",
                                   extra_activity3_duration*extra_activity3_frequency,",",
                                   extra_activity4_duration*extra_activity4_frequency,",",
                                   extra_activity5_duration*extra_activity5_frequency), by=id]

activityCompositesColums = atecFuncCreateActivityComposite(processedQuestionnaire2, 
                                                           nameColumn = "extracurriculumActivitiesNames",
                                                           durationColumn = "extracurriculumActivitiesDurations",
                                                           defaultactivityDataFile = file.path(pathToDataFolder, "utility/activities/activityCorrespondanceTable.csv"))
processedQuestionnaire2 = cbind(processedQuestionnaire2, activityCompositesColums)

```

Wellbeing composites will be calculated as follows :

```{r, warning=F}
welbeingColumnNames = paste0("welfareQ",1:6) 

processedQuestionnaire2[,(welbeingColumnNames) := lapply(.SD, function(feature) { 
    as.numeric(as.character(factor(feature, levels = c(1,2,3,4,5), labels = c(1,2,3,4,5)))) }), 
    .SDcols = (welbeingColumnNames)]

processedQuestionnaire2[, wellbeing_score := 30-(welfareQ1+welfareQ2+welfareQ3+welfareQ4+welfareQ5+welfareQ6)]
```

Structure of processed data as follows :

```{r, echo=F, warning=F}
str(processedQuestionnaire2)
```


### 5. rawDataQuestionnaireConnersTeacher.csv
```{r, warning=F}
rawQuestionnaireTeacher = read.csv(file.path(pathToDataFolder,"raw/questionnaires/rawDataQuestionnaireConnersTeacher.csv"))
```

This questionnaire corresponds to the teacher's part of the conners test. Score were alculated in the raw data. The formula is present in the xls file and seems approriatly calculated. Hence it will not be recalculated. Only 125 subject were scored through by their teacher.

This file contains all answered data from questionnaire 2 for each subject, as follows :
```{r, echo=F, warning=F}
colnames(rawQuestionnaireTeacher)
```

#### Processing data

```{r, echo=F, warning=F}
processedQuestionnaireTeacher = as.data.table(rawQuestionnaireTeacher)
```

Variables will be renamed as follows :
```{r, warning=F}
setnames(processedQuestionnaireTeacher, c("id", paste0("connersTeacherQ", 1:28)
                                          , "connersTeacherBehavioralScore"
                                          , "connersTeacherImpulsivityScore"
                                          , "connersTeacherAttentionScore"
                                          , "connersTeacherTotalScore"))

cnames = c(conners_teacher_questions)
# All conner questions need to be recoded to scale from 0 to 3
processedQuestionnaireTeacher[, (cnames) := lapply(.SD, function(feature) { 
    as.numeric(as.character(factor(feature, levels = c(1,2,3,4), labels = c(0,1,2,3) ))) }), .SDcols = (cnames)]


conners_teacher_questions_for_conduct = c(4,5,6,10,11,12,23,27)
conners_teacher_questions_for_hyperactivity = c(1,2,3,8,14,15,16)
conners_teacher_questions_for_inattention = c(7,9,18,20,21,22,26,28)


processedQuestionnaireTeacher[, conners_teacher_conduct_score := sum(as.numeric(.SD)), .SDcols= conners_teacher_questions[conners_teacher_questions_for_conduct], by="id"]

processedQuestionnaireTeacher[, conners_teacher_hyperactivity_score := sum(as.numeric(.SD)), .SDcols= conners_teacher_questions[conners_teacher_questions_for_hyperactivity], by="id"]

processedQuestionnaireTeacher[, conners_teacher_inattention_score := sum(as.numeric(.SD)), .SDcols= conners_teacher_questions[conners_teacher_questions_for_inattention], by="id"]

processedQuestionnaireTeacher[, conners_teacher_total_score := sum(as.numeric(.SD)), .SDcols= conners_teacher_questions, by="id"]

```


### 6. rawDataTaskD2.csv
```{r, warning=F}
rawTaskD2 = read.csv(file.path(pathToDataFolder,"raw/d2/rawDataTaskD2CleanedUp.csv"))
```

This files contains raw data about the D2 task, as follows :

```{r, echo=F, warning=F}
colnames(rawTaskD2)
```

#### Processing Data

```{r, echo=F, warning=F}
processedTaskD2 = as.data.table(rawTaskD2)
```

We have to be carefull as it is stated in Isabel's material and method : _First Row is for training_

```{r, warning=F}
setnames(processedTaskD2, c("sid"), c("id"))
processedTaskD2 = processedTaskD2[, c("date", "hour") := NULL]
```

Initially one composite feature was calculated: 

* KL = nb Total Item Parsed - nb Errors. 
* This marker represents an absolute difference between a global mesure of processing speed (number of item parsed) and general mesure of a failure of attention process containing both impulsivity error (comission) and inatention error (omission)

We added other composite mesures of :

* Processing speed 
    + Using trend analysis as a marker of attention stability
    + Cons of trend analysis in that context : 
    + too little time points...7 lines
    + what model to choose ? Quadratic would account for learning curves... maybe best
    + First methods with total variation
        + speedVariability = total variation of number of item = sum of the absolute differences between number of item parsed at each line, gives a sense of the variability of speed performance, and an indirect marker of sustained attention. But it doesnt differenciate between a variability due to learning or loss of attention
        + evolutionOfPErformance = general evolution of performance = sum of the differences (not absolute) between number of item parsed at each line, can be simplified by the difference between first and last line, but loose information of the time course of the variation. For example loose information of an initial increase of performance due to learning folowed by a loss of attention.
        +  attentionStability = evolutionOfPerformance / speedVariability - permit to get a better estimate of the stability of attention by normaliyiing global evolution of performance with variability. Values are between -1 and 1. The closer it is to its limits, the closer the variation of speed is explained by a first order linear model (constant learning, of progressive loss of interest). For example :
                + if speed increase linearly with the number of lines with constant pace then attention stability will be 1.
                + speed = a + numberOfLine + C
                + speedEnd-speedBegining = evolutionOfPerf
                + evolutionOfPerf = a(numberofline-1) = speedVariability  
                + If evolution of performance is erratic with ups and down, its mean converge to 0 and stability also. 
                + If speedvariability converge to 0, so does evolution of performance that is always inferior or equal to variability. Of course zero variability is not defined. Could be manually coded as 1.
                + If performance are getting linearly worse (a<0) stability is negative between -1 and 0.
* Second method using omdel fitting
    + Curve fit to a linear model and use least square calculus to evaluate the distance from the curve
    + either first order or second order (secondary loss of interest or )
    + variance could also work but we loose information on time course, as variance gives a general sense of speed variability. And it cant be used in the formula for attention stability
* Error related
    + Type
        + Impulsivity Inatention Index 
            + II = (commissionErrors-omissionErrors)/totalErrors 
            + A scalar value scale placing each subject between an impulsive type (errorTypeIndex = 1) or an inattention type (errorTypeIndex = 0)
            + This index might separate two populations based on error type and show an evolution over time. We could expect for example that younger children are more impulsive and gradually become able to develop control
    + Hits Ratio
        + H = Nb Hits / (Number of Targets = Number Hits + Nb Omission Error)
    + FA Ratio 
        + FAr = FA / ( FA + Omission )
    + __Sensitivity index d'__
        + d' = Z(H) - Z(FAr)
    + Time course of errors ... was not computed.
    
    We need to recalculate number of errors without the first line that served as training.

```{r, warning=F}

commissionErrorColumnNames = paste0("nb_co_L",2:9)
ommissionErrorColumnNames = paste0("nb_om_L",2:9)
itemReadColumnNames = paste0("nb_items_L",2:9)
numberOfHitsColumnNames = paste0("hits_L",2:9)
hitsAndOmmissionCols =  c(paste0("hits_L",2:9), paste0("nb_om_L",2:9))

processedTaskD2[, d2CommissionErrorsTotal := sum(.SD), .SDcols = commissionErrorColumnNames, by=id]
processedTaskD2[, d2OmmissionErrorsTotal := sum(.SD), .SDcols = ommissionErrorColumnNames, by=id]
processedTaskD2[, d2HitsTotal := sum(.SD), .SDcols = numberOfHitsColumnNames, by=id]
processedTaskD2[, d2TotalError := d2CommissionErrorsTotal + d2OmmissionErrorsTotal, by=id]
processedTaskD2[, d2TotalItemRead := sum(.SD), .SDcols = itemReadColumnNames, by=id]
processedTaskD2[, d2TotalTargets := sum(.SD), .SDcols=hitsAndOmmissionCols,by=id]
processedTaskD2[, d2TotalNonTargets := d2TotalItemRead-d2TotalTargets, .SDcols=hitsAndOmmissionCols,by=id]

processedTaskD2[, d2KL := d2TotalItemRead - d2TotalError, by=id]
processedTaskD2[, d2ImpulsivityInattentionIndex := (d2CommissionErrorsTotal - d2OmmissionErrorsTotal)/d2TotalError, by=id]
processedTaskD2[, d2HitsRatioTotal := d2HitsTotal/d2TotalTargets, by=id]
processedTaskD2[, d2FalseAlarmsRatioTotal := d2CommissionErrorsTotal/d2TotalNonTargets, by=id]

# Not a good way to do it... 
#getZScore = function (x, mean, sd)
# {
#     return((x-mean)/sd) 
# }
# 
# meanFAr = mean(processedTaskD2[, d2FalseAlarmsRatioTotal])
# sdFAr = sd(processedTaskD2[, d2FalseAlarmsRatioTotal])
# meanH = mean(processedTaskD2[, d2HitsRatioTotal])
# sdH = sd(processedTaskD2[, d2HitsRatioTotal])

getDPrimeFromANotA = function (d2HitsTotal, d2TotalTargets, d2CommissionErrorsTotal, d2TotalNonTargets)
{
    if (!is.na(d2HitsTotal)&!is.na(d2TotalTargets)&!is.na(d2CommissionErrorsTotal)&!is.na(d2TotalNonTargets))
    {
        ## Prevent bug if Hits and FA is 0
        if (d2HitsTotal==0)
            d2HitsTotal = 1
        if (d2CommissionErrorsTotal==0)
            d2CommissionErrorsTotal = 1
        
        if (d2HitsTotal==d2TotalTargets)
            d2TotalTargets = d2TotalTargets+1
        
        return(AnotA(d2HitsTotal,d2TotalTargets,d2CommissionErrorsTotal,d2TotalNonTargets)[[1]][[1]])
    }
    
}
## Get D Prime from the sensR anota object
processedTaskD2[, d2DPrime := getDPrimeFromANotA(d2HitsTotal, d2TotalTargets, d2CommissionErrorsTotal, d2TotalNonTargets), 
                by=id]  

processedTaskD2[, d2ItemReadTotalVariation := atecFuncTimeSeriesTotalVariation(as.integer(paste0(.SD))), .SDcols = itemReadColumnNames, by=id]
processedTaskD2[, d2AttentionStabilityComposite := (get(tail(itemReadColumnNames,1)) - get(itemReadColumnNames[1]))/ d2ItemReadTotalVariation, by=id]

commissionErrorColumnNames = paste0("nb_co_L",1:9)
ommissionErrorColumnNames = paste0("nb_om_L",1:9)
itemReadColumnNames = paste0("nb_items_L",1:9)
numberOfHitsColumnNames = paste0("hits_L",1:9)

for (i in 1:9)
{
    processedTaskD2[, c(paste0("d2HitsRatioL",i)) := get(numberOfHitsColumnNames[i]) /
                        (get(numberOfHitsColumnNames[i]) +
                             get(ommissionErrorColumnNames[i])), by=id]
}

```


### 7. rawDataTaskLachaux.csv

Lachaux data file contains 11 features :
1) *nb* - a unique id for each trials grouping all subjects together
2) *kid.id* - subject id
3) *test.session* - there was two session (for each subject?)
4) *attempt* - from 1 to 3 - to verify what it means dabr josu chva cory and timo have > 1
5) *trial* - in general from 1 to 59 for some (those with attempts > 1) it goes higher higher
6) *pre_target*	- contains the letter of interest
7) *target*	- the sequence of letters
8) *target_position* - 
position of the target inside the sequence 
look in the literature if that is known to be important
9) *signal*	- the correct response (T/F)
10) *choice* - the subject response (T/F)
11) *rt* - reaction time - look in the litterature if a certain limit of reaction time is known to reflect conscious choice

```{r, warning=F}
rawTaskLachaux = read.csv(file.path(pathToDataFolder,"raw/lachaux/rawDataTaskLachaux.csv"))
lachaux = as.data.table(rawTaskLachaux)
lachaux = lachaux[rt>150&rt<3000,]
## Left join with relevant data for correlation



setDT(lachaux)[processedSubjectsGeneralInformations, `:=`(gender = i.gender, group = i.group), on = c(kid.id = "id")]

```

14 of the subjects that passed the test are not registered in the general information files, and we dont have information on them.

```{r, echo=F,warning=F}

unique(lachaux[is.na(gender), kid.id]) ## all those kids were not entered in the information questionnaire
```

#### Processing Data

For each subject we will calculate :
1) lachauxCorrectRatio
2) lachauxMeanRT
3) lachauxMedianRT
4) lachauxSdRT 
5) lachauxCovRT
6) lachauxTrialFocusedRatio
7) lachauxTrialIntermediateRatio
8) lachauxTrialNotFocusedRatio

```{r, warning=F}


medianForGroup = function (x) {
    return(lachaux[group==x, as.double(median(rt, na.rm = T))])
}

lachauxById = lachaux[, list(
    percentCorrect = NROW(signal[signal==choice])/NROW(signal), 
    meanRT = mean(rt, na.rm = T), 
    medianRT = as.double(median(rt, na.rm = T)), 
    sdRT = sd(rt, na.rm = T), 
    covRT = sd(rt, na.rm = T)/mean(rt, na.rm = T),
    trialFocusedRatio = atecFuncLachauxGetFocusedTrials(rt, 
                                                        medianForGroup(group),
                                                        "nFR"),
    trialLargeFocusedRatio = atecFuncLachauxGetFocusedTrials(rt, 
                                                        medianForGroup(group),
                                                        "nLFR"),
    trialIntermediateRatio = atecFuncLachauxGetFocusedTrials(rt, 
                                                             medianForGroup(group),
                                                             "nIR"),
    trialNotFocusedRatio = atecFuncLachauxGetFocusedTrials(rt, 
                                                           medianForGroup(group),
                                                           "nNFR")),
    by=c("kid.id", "gender", "group")]


## Add it to the processedSubjectsGeneralInformations for future merging 

setDT(processedSubjectsGeneralInformations)[lachauxById, `:=`(lachauxCorrectRatio = i.percentCorrect,
                                                              lachauxMeanRT = i.meanRT,
                                                              lachauxMedianRT = i.medianRT,
                                                              lachauxSdRT = i.sdRT,
                                                              lachauxCovRT = i.covRT,
                                                              lachauxTrialFocusedRatio = i.trialFocusedRatio,
                                                              lachauxTrialIntermediateRatio = i.trialIntermediateRatio,
                                                              lachauxTrialNotFocusedRatio = i.trialNotFocusedRatio), 
                                            on = c(id = "kid.id")]
```

## SART

SART raw data file has been preprocessed in excel. 

Columns are as follows :

1) "numObs" : holds a identifier for each observations accross subjects and sessions
2) "id" : subject id
3) "testSession" : 1 or 2 - there was two test sessions
4) "trial" : the number of observation for a subject in a session                    
5) "attempt" : 1 - 2 - 3 - 4 - NA :: Increases if subject stop tasks during a session and restarts ?
6) "stimulusOnset" : timestamp for a session holding the time of presentation of stimulus
7) "stimulus" : hold the string of the stimulus ["1" - "9"]
8) "target" : the good response T or F                    
9) "rt" : reaction time after stimulus presentation
10) "numberOfClicksPerWindow" : total number of clicks in a session
11) "response" : subject response  

```{r, warning=FALSE, message=FALSE}
sartData = as.data.table(read.csv(file.path(pathToDataFolder, "raw/sart/preProcessedDataTaskSart.csv")))
sartData[, sartNumNA := as.double(ntrue(is.na(rt))), by=id]
sartData[, sartNumTrials := as.double(max(trial, na.rm = T)), by=c("id","testSession")]
sartData[, sartNumTrials := sum(unique(sartNumTrials, na.rm = T)), by=c("id")]

# Recode for the response - all trial with a rt are considered as a true response (the subject clicked the screen)
sartData[, response:=T]
sartData[is.na(rt), response:=F]

sartData[, postTarget := slideVector(stimulus,1,0)==3, by=c("id","testSession")]
sartData[, moreThanAClick := ((numberOfClicksPerWindow-slideVector(numberOfClicksPerWindow,1,0)) > 1),  by=c("id","testSession")]




sartData[, `:=`(
    sartFullPercentCorrect = NROW(target[target!=response])/NROW(target),
    sartFullNofOmission = NROW(target[target==TRUE&response==TRUE]),
    sartFullNofCommission = NROW(target[target==FALSE&response==FALSE]),
    sartFullMeanRT = mean(rt, na.rm = T), 
    sartFullMedianRT = as.double(median(rt, na.rm = T)), 
    sartFullSdRT = sd(rt, na.rm = T), 
    sartFullCovRT = sd(rt, na.rm = T)/mean(rt, na.rm = T),
        
    sartFullMeanRTCorrectTrial = mean(rt[target!=response], na.rm = T),
    sartFullMedianRTCorrectTrial = as.double(median(rt[target!=response], na.rm = T)),
    sartFullSdRTCorrectTrial = as.double(sd(rt[target!=response], na.rm = T)),
    sartFullCovRTCorrectTrial = as.double(sd(rt[target!=response], na.rm = T)/mean(rt[target!=response], na.rm = T)),
        
    sartFullMeanRTPostTargetTrial = mean(rt[postTarget==T], na.rm = T),
    sartFullMedianRTPostTargetTrial = as.double(median(rt[postTarget==T], na.rm = T)),
    sartFullSdRTPostTargetTrial = as.double(sd(rt[postTarget==T], na.rm = T)),
    sartFullCovRTPostTargetTrial = as.double(sd(rt[postTarget==T], na.rm = T)/mean(rt[postTarget==T], na.rm = T)),
    
    sartHalfOnePercentCorrect = NROW(target[target!=response&trial<=160])/NROW(target[trial<=160]),
    sartHalfOneNofOmission = NROW(target[target==TRUE&response==TRUE&trial<=160]),
    sartHalfOneNofCommission = NROW(target[target==FALSE&response==FALSE&trial<=160]),
    sartHalfOneMeanRT = mean(rt[trial<=160], na.rm = T), 
    sartHalfOneMedianRT = as.double(median(rt[trial<=160], na.rm = T)), 
    sartHalfOneSdRT = sd(rt[trial<=160], na.rm = T), 
    sartHalfOneCovRT = sd(rt[trial<=160], na.rm = T)/mean(rt[trial<=160], na.rm = T),
    sartHalfTwoPercentCorrect = NROW(target[target!=response&trial>160])/NROW(target[trial>160]), 
    sartHalfTwoNofOmission = NROW(target[target==TRUE&response==TRUE&trial>160]),
    sartHalfTwoNofCommission = NROW(target[target==FALSE&response==FALSE&trial>160]),
    sartHalfTwoMeanRT = mean(rt[trial>160], na.rm = T), 
    sartHalfTwoMedianRT = as.double(median(rt[trial>160], na.rm = T)), 
    sartHalfTwoSdRT = sd(rt[trial>160], na.rm = T), 
    sartHalfTwoCovRT = sd(rt[trial>160], na.rm = T)/mean(rt[trial>160], na.rm = T)),
    #     trialFocusedRatio = atecFuncLachauxGetFocusedTrials(rt, 
    #                         medianForGroup(group),
    #                         "nFR"),
    #     trialIntermediateRatio = atecFuncLachauxGetFocusedTrials(rt, 
    #                         medianForGroup(group),
    #                         "nIR"),
    #     trialNotFocusedRatio = atecFuncLachauxGetFocusedTrials(rt, 
    #                         medianForGroup(group),
    #                         "nNFR")),
    by=c("id")]

sartData = sartData[!duplicated(id), list(id, 
                                          sartNumTrials, 
                                          sartNumNA, 
                                          sartFullPercentCorrect, 
                                          sartFullNofOmission,
                                          sartFullNofCommission,
                                          sartFullMeanRT,
                                          sartFullMedianRT,
                                          sartFullSdRT,
                                          sartFullCovRT,
                                          sartFullMeanRTCorrectTrial,
                                          sartFullMedianRTCorrectTrial,
                                          sartFullSdRTCorrectTrial,
                                          sartFullCovRTCorrectTrial,
                                          sartFullMeanRTPostTargetTrial,
                                          sartFullMedianRTPostTargetTrial,
                                          sartFullSdRTPostTargetTrial,
                                          sartFullCovRTPostTargetTrial,
                                          sartHalfOnePercentCorrect,
                                          sartHalfOneNofOmission,
                                          sartHalfOneNofCommission,
                                          sartHalfOneMeanRT,
                                          sartHalfOneMedianRT,
                                          sartHalfOneSdRT,
                                          sartHalfOneCovRT,
                                          sartHalfTwoPercentCorrect,
                                          sartHalfTwoNofOmission,
                                          sartHalfTwoNofCommission,
                                          sartHalfTwoMeanRT,
                                          sartHalfTwoMedianRT,
                                          sartHalfTwoSdRT,
                                          sartHalfTwoCovRT)]
```

## CATCH THE WOLF

Catch the wolf task data is contained in a RData file called ATEC_cw.RData which itself contain a data.frame called cw:
1. "sid" - subject id - renamed to id
2. "gender" - string - male or female 
3. "year" - integer - year of the test 
4. "month" - integer - month of the test
5. "day" - integer - day of the test
6. "hour" - integer - hour of the test
7. "minute" - integer - minute of the test
8. "laptop" - string - id of device used for the test
9. "block" - string - ?
10. "trial" - integer - trial number
11. "wolfPos" - integer - [0-3] representing the 4 corners of the screen
12. "choice" - integer - [0-3] subject prediction of the next wolf position
13. "RT" - integer - reaction time, varies a lot 10^7 to +10^7
14. "correct" - integer - -1 0 1 -1 seems to be uncorrect, 1 is correct, 0 is ?


41 subject are not present in or other files and will be deleted.
```{r, warning=F, echo=F } 
load(file.path(pathToDataFolder,"raw/catch_the_wolf/ATEC_cw.Rdata"))
cwData = as.data.table(cw)

## Filter id that exist in our questionnaire
cwData = cwData[sid%in%unique(processedSubjectsGeneralInformations[,id])]

# add 1 to block so we have 1 to 4
cwData[,block := block+1] 

# initialize CWObject with the data
states = 0:3
CWO = CWObject$new()
CWO$setData(data = cwData, states = states, idVarname = "sid")

CWO$addComputedData(computation = "accuracy", sufix = 'sliding', type = 'sliding')
 
CWO$addLearnerType("omnicient")

avgData = copy(CWO$getAveragedData()) 

```

# B. Merging Processed Data
In order to obtain one final data.table containing all the relevant information about a subject each data.table will be merged over the "id" column.
Only 135 children over the 156 sampled took the first questionnaire. 

```{r, warning=F}
## unique(processedSubjectsGeneralInformations[order(id),id]) -- 156
## unique(processedCodedTasksAndQuestionnaires[order(id),id]) -- 156
## unique(processedQuestionnaire1[order(id),id]) -- 135

preprocessed_data = merge(processedSubjectsGeneralInformations, 
                                processedCodedTasksAndQuestionnaires,
                                by="id",all.x = TRUE)

preprocessed_data = merge(preprocessed_data, 
                                processedQuestionnaire1,
                                by="id",all.x = TRUE)

preprocessed_data = merge(preprocessed_data, 
                                processedQuestionnaire2,
                                by="id",all.x = TRUE)

preprocessed_data = merge(preprocessed_data, 
                                processedQuestionnaireTeacher,
                                by="id",all.x = TRUE)

preprocessed_data = merge(preprocessed_data, 
                                processedTaskD2,
                                by="id",all.x = TRUE)

preprocessed_data = merge(preprocessed_data, 
                                sartData,
                                by="id",all.x = TRUE)

preprocessed_data = merge(preprocessed_data,
                                avgData,
                                by="id",all.x = TRUE)
```

# Computation on merged data :
We compute the composite teacher / parent conners score 

```{r composite conners}
# compute total score with parent 
preprocessed_data[, conners_parent_teacher_total_score := sum(as.numeric(.SD)), .SDcols= c(conners_parent_questions[parent_questions_for_total_score], conners_teacher_questions[teacher_questions_for_total_score]), by="id"]

preprocessed_data[, c(conners_parent_questions[parent_questions_for_total_score], conners_teacher_questions[teacher_questions_for_total_score]), with=F]
```

A composite variable for age-meanAgeOfClass to discriminate kids that are in advance or late compared to their peers.

```{r, echo=F, warning=F}
#preprocessed_data[, ageClassMean := mean(age), by="schoolYear"]
#preprocessed_data[, ageDifferenceFromMean := age-ageClassMean, by="id"]

dateOfTest = dmy("18-01-2015")
preprocessed_data[, ageInSeconds := period_to_seconds(as.period(dmy(dateOfBirth)%--%dateOfTest)), by="id"]
preprocessed_data[, ageClassMeanInSeconds := mean(ageInSeconds), by="schoolYear"]

preprocessed_data[, ageInDaysFromBirthDate := ageInSeconds/86400, by="id"]
preprocessed_data[, ageInYearsFromBirthDate := ageInSeconds/3.154e+7, by="id"]

# preprocessed_data[, ageDifferenceFromMeanInSecondsStandardized := (ageInSeconds-ageClassMeanInSeconds), by="id"]
# 
# 
# standardDeviationOfAgeDifferenceInSeconds = preprocessed_data[, sd(ageDifferenceFromMeanInSecondsStandardized)] 
# 
# preprocessed_data[, ageDifferenceFromMeanInSecondsStandardized := (ageInSeconds-ageClassMeanInSeconds)/standardDeviationOfAgeDifferenceInSeconds, by="id"]

```

# Recode direction of speed and sd to make more sense in the PCA as speed and regularity

```{r, echo=F, warning=F}
preprocessed_data[, c("sartFullSpeed", "sartFullRegularity", "lachauxSpeed", "lachauxRegularity") := list(-sartFullMedianRTCorrectTrial, -sartFullSdRTCorrectTrial,  -lachauxMedianRT, -lachauxCovRT)]

for (i in c(1:4, "All")) {
        preprocessed_data[,  paste0("cwSpeedBlock",i) := list(-get(paste0("cwMedianRTBlock",i)))]
}

``` 

# Dimensionality reduction PCA

will depends which subject to include. It will be done separately.

# Then Save the data

```{r, warning=F} 
 save(preprocessed_data, file = file.path(pathToDataFolder, "computed/preprocessed/preprocessed_data.Rdata"))
```

